{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7c5732e0",
   "metadata": {},
   "source": [
    "# Explainable Outcome Prediction - Hyperparameter Tuning\n",
    "\n",
    "- Author: David Steiner\n",
    "- December 2021"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7c69fdb6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import math\n",
    "import keras\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import save_model, model_from_json, Sequential\n",
    "from tensorflow.keras.layers import Dense, LSTM, GRU, Dropout, BatchNormalization\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from tensorflow.keras.layers import Conv1D, MaxPooling1D, Flatten, Dense, Dropout, Activation\n",
    "from kerastuner.tuners import RandomSearch, Hyperband, BayesianOptimization\n",
    "from kerastuner.engine.hyperparameters import HyperParameters\n",
    "from pickle import dump,load\n",
    "\n",
    "import pandas as pd\n",
    "pd.set_option(\"display.max_columns\", 200)\n",
    "pd.set_option(\"display.max_rows\", 100)\n",
    "import numpy as np\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "from prep_custom import get_dataset_settings, create_trace_bucket,remove_features,remove_events,split_data_temporal\n",
    "from prep_custom import replace_missing_cols,prepare_ml_train_test, aggregate_data, group_infrequent_features, cut_trace_before_activity\n",
    "from prep_custom import scale_data, scale_data, one_hot_encode, prepare_dl_train_test, define_binary_outcome_label\n",
    "from pred_custom import get_evaluation_metrics, train_model, plot_train_history, get_cnn_clf, get_lstm_clf, get_gru_clf "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a9681d7",
   "metadata": {},
   "source": [
    "## Dataset Selection and Outcome Definition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "49fe77d7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Categoric Event Attributes: 6 ['concept:name', 'org:resource', 'Action', 'lifecycle:transition', 'Accepted', 'Selected'] \n",
      "\n",
      "Numeric Event Attributes: 13 ['CreditScore', 'FirstWithdrawalAmount', 'MonthlyCost', 'NumberOfTerms', 'OfferedAmount', 'timesincelastevent', 'timesincecasestart', 'timesincemidnight', 'event_nr', 'month', 'weekday', 'hour', 'open_cases'] \n",
      "\n",
      "Categoric Case Attributes: 3 ['case:ApplicationType', 'case:LoanGoal', 'EventOrigin'] \n",
      "\n",
      "Numeric Case Attributes: 1 ['case:RequestedAmount'] \n",
      "\n",
      "Dataset Shape (1202267, 26)\n"
     ]
    }
   ],
   "source": [
    "data, case_id_col, activity_col, timestamp_col, label_col, resource_col, event_categorical_attributes, event_numeric_attributes, case_categorical_attributes, case_numeric_attributes, static_cols, dynamic_cols, cat_cols = get_dataset_settings('BPIC17')\n",
    "attributes = [event_categorical_attributes, case_categorical_attributes, event_numeric_attributes, case_numeric_attributes]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a1dae7bf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Set labels to 1 for Outcome: Long Running Case\n",
      "label\n",
      "0    23135\n",
      "1     8374\n",
      "Name: case:concept:name, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# Select dataset and outcome\n",
    "data_labeled, drop_events_list, dl_attributes = define_binary_outcome_label(data, attributes, outcome_label='BPIC17-LongRunningCases')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2096e395",
   "metadata": {},
   "source": [
    "## Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "62f7fcab",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Binning and Grouping infrequent factor levels to: Other_infrequent\n",
      "Making predictions after 15 completed events\n",
      "Making predictions at time after cases have started on average:  57.7 hours/ 2.403 days\n",
      "Making predictions at time before cases have finished on average:  467.9 hours/ 19.5 days\n",
      "\n",
      " Shape: (472433, 26)\n",
      "Features removed:  1\n",
      "\n",
      " Events included:\n",
      "['A_Create Application' 'A_Submitted' 'W_Handle leads'\n",
      " 'W_Complete application' 'A_Concept' 'A_Accepted' 'O_Create Offer'\n",
      " 'O_Created' 'O_Sent (mail and online)' 'W_Call after offers' 'A_Complete'\n",
      " 'O_Cancelled' 'W_Validate application' 'O_Sent (online only)' 'A_Denied'\n",
      " 'O_Refused' 'A_Cancelled' 'W_Assess potential fraud'\n",
      " 'W_Shortened completion ']\n"
     ]
    }
   ],
   "source": [
    "data_labeled_grouped = group_infrequent_features(data_labeled, max_category_levels = 15)\n",
    "trace_bucket = create_trace_bucket(data_labeled_grouped, 15)\n",
    "trace_bucket = remove_features(trace_bucket, dl_attributes[0], dl_attributes[1], dl_attributes[2], dl_attributes[3])\n",
    "trace_bucket = remove_events(data_labeled, trace_bucket, drop_events_list)\n",
    "\n",
    "train_raw, test_raw = split_data_temporal(data=trace_bucket, train_ratio=0.8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "31c38f55",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[] []\n"
     ]
    }
   ],
   "source": [
    "#RNN Preprocessing\n",
    "train, test = scale_data(train_raw, test_raw)\n",
    "train = one_hot_encode(train)\n",
    "test = one_hot_encode(test)\n",
    "train, test = replace_missing_cols(train, test)\n",
    "\n",
    "X_train, y_train, feature_names = prepare_dl_train_test(train, 15)\n",
    "X_test, y_test, feature_names = prepare_dl_train_test(test, 15)\n",
    "\n",
    "#y_train = to_categorical(y_train, 2)\n",
    "#y_test = to_categorical(y_test, 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a42f166",
   "metadata": {},
   "outputs": [],
   "source": [
    "#ML Preprocessing\n",
    "sequence_encoding_setting = dict(case_id_col=case_id_col, \n",
    "                          activity_col=activity_col,\n",
    "                          label_col=label_col,\n",
    "                          case_numeric_attributes=case_numeric_attributes,\n",
    "                          case_categorical_attributes=case_categorical_attributes,\n",
    "                          event_numeric_attributes=event_numeric_attributes,\n",
    "                          event_categorical_attributes=event_categorical_attributes,\n",
    "                            d_event_sequence = False,\n",
    "                            d_event = True,\n",
    "                            d_event_categorical = True,\n",
    "                            d_case_categorical =True,\n",
    "                            one_hot_case_categorical = True,   \n",
    "                            d_event_numeric = True,\n",
    "                            d_case_numeric = True)\n",
    "\n",
    "\n",
    "train = aggregate_data(train_raw, \n",
    "                       **sequence_encoding_setting)\n",
    "\n",
    "test = aggregate_data(test_raw, \n",
    "                      **sequence_encoding_setting)\n",
    "\n",
    "train, test = replace_missing_cols(train, test)\n",
    "\n",
    "X_train_ml, y_train_ml, X_test_ml, y_test_ml = prepare_ml_train_test(train,test, balanced=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a24b7355",
   "metadata": {},
   "outputs": [],
   "source": [
    "#DNN Preprocessing\n",
    "\n",
    "#Feature Scaling for DNN\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "#Split into feature (X) and target (y) variables\n",
    "x_train_numeric = X_train_ml\n",
    "y_train_numeric = y_train_ml[label_col].astype(np.int64)\n",
    "x_test_numeric = X_test_ml\n",
    "y_test_numeric = y_test_ml[label_col].astype(np.int64)\n",
    "\n",
    "\n",
    "#Apply standardization on numerical features\n",
    "num_cols = x_train_numeric.columns.values\n",
    "\n",
    "for col in num_cols: \n",
    "    scale = StandardScaler().fit(x_train_numeric[[col]])   \n",
    "    x_train_numeric[col] = scale.transform(x_train_numeric[[col]])\n",
    "    x_test_numeric[col] = scale.transform(x_test_numeric[[col]])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0853f5df",
   "metadata": {},
   "source": [
    "## DL Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "c5c879b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tune_gru(hp):\n",
    "    model = Sequential()\n",
    "    model.add(GRU(hp.Int('input_unit',min_value=32,max_value=512,step=32),return_sequences=True, input_shape=(X_train.shape[1],X_train.shape[2])))\n",
    "    \n",
    "    for i in range(hp.Int('n_layers', 1, 2)):\n",
    "        model.add(GRU(hp.Int(f'lgru_{i}_units',min_value=32,max_value=512,step=32),return_sequences=True))\n",
    "        \n",
    "    model.add(GRU(hp.Int('layer_final',min_value=32,max_value=512,step=32)))\n",
    "    \n",
    "    model.add(Dropout(hp.Float('Dropout_rate',min_value=0,max_value=0.5,step=0.1)))\n",
    "    \n",
    "    model.add(Dense(1, activation=hp.Choice('dense_activation', values=['softmax', 'sigmoid'])))\n",
    "    \n",
    "    model.compile(loss='binary_crossentropy', optimizer='sgd',metrics = ['AUC', 'acc'])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "174485f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tune_lstm(hp):\n",
    "    model = Sequential()\n",
    "    model.add(LSTM(hp.Int('input_unit',min_value=32,max_value=512,step=32),return_sequences=True, input_shape=(X_train.shape[1],X_train.shape[2])))\n",
    "    \n",
    "    for i in range(hp.Int('n_layers', 1, 2)):\n",
    "        model.add(LSTM(hp.Int(f'lstm_{i}_units',min_value=32,max_value=512,step=32),return_sequences=True))\n",
    "        \n",
    "    model.add(LSTM(hp.Int('layer_final',min_value=32,max_value=512,step=32)))\n",
    "    \n",
    "    model.add(Dropout(hp.Float('Dropout_rate',min_value=0,max_value=0.5,step=0.1)))\n",
    "    \n",
    "    model.add(Dense(1, activation=hp.Choice('dense_activation', values=['softmax', 'sigmoid'])))\n",
    "    \n",
    "    model.compile(loss='binary_crossentropy', optimizer='sgd',metrics = ['AUC', 'acc'])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df20b711",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tune_cnn(hp):\n",
    "    model = Sequential()\n",
    "    \n",
    "\n",
    "    model.add(Conv1D(hp.Int('input_unit', min_value=1, max_value=16, step=2), \n",
    "                     kernel_size=hp.Int('input_kernel', min_value=10, max_value=20, step=2), \n",
    "                     padding='same',\n",
    "                     input_shape=(X_train.shape[1], X_train.shape[2]), \n",
    "                     #activation=hp.Choice('input_layer_activation', values=['relu', 'tanh'])))\n",
    "                     activation='tanh'))\n",
    "\n",
    "    #for i in range(hp.Int('n_cnn_layers', 1, 2)):    \n",
    "    i=1\n",
    "    model.add(Conv1D(hp.Int(f'hidden_{i}_units', min_value=1, max_value=16, step=4), \n",
    "                 kernel_size=hp.Int(f'hidden_{i}_kernel', min_value=1, max_value=4, step=1), \n",
    "                 input_shape=(X_train.shape[1], X_train.shape[2]), \n",
    "                 activation=hp.Choice(f'hidden_{i}_activation', values=['relu', 'tanh'])))\n",
    "    \n",
    "    model.add(Dropout(hp.Float('Dropout_rate_input',min_value=0,max_value=0.5,step=0.1)))\n",
    "    model.add(MaxPooling1D(pool_size=2))\n",
    "    model.add(Flatten())\n",
    "       \n",
    "    for i in range(hp.Int('n_layers', 4, 6)):\n",
    "        model.add(Dropout(hp.Float(f'Dropout_rate_{i}',min_value=0,max_value=0.5,step=0.1)))\n",
    "        model.add(Dense(hp.Int(f'dense_{i}', min_value=2, max_value=256, step=16)))\n",
    "        #model.add(Activation(hp.Choice(f'dense_{i}_activation', values=['relu', 'tanh'])))\n",
    "        model.add(Activation('tanh'))\n",
    "        \n",
    "    #model.add(Dense(y_train.shape[1], activation=hp.Choice('final_activation', values=['softmax', 'sigmoid'])))\n",
    "    model.add(Dense(1, activation='sigmoid'))\n",
    "    \n",
    "    model.compile(loss='binary_crossentropy', #crossentropy\n",
    "                  metrics=['AUC', 'acc', 'loss'],\n",
    "                  optimizer='sgd')\n",
    "              \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10d10fc5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tune_dnn(hp):\n",
    "    model = Sequential()\n",
    "    model.add(Dense(hp.Int('input_unit', min_value=32, max_value=1024, step=64), \n",
    "                    input_dim=x_train_numeric.shape[1], \n",
    "                    #activation=hp.Choice(f'input_activation', values=['relu', 'tanh'])))\n",
    "                    activation='relu')) \n",
    "    \n",
    "    for i in range(hp.Int('n_layers', 1, 4)):\n",
    "        model.add(Dropout(hp.Float(f'layer_{i}_dropout',min_value=0,max_value=0.5,step=0.1)))\n",
    "        model.add(BatchNormalization())\n",
    "        model.add(Dense(hp.Int(f'layer_{i}_dense', min_value=32, max_value=1024, step=64), \n",
    "                        #hp.Choice(f'layer_{i}_activation', values=['relu', 'tanh'])\n",
    "                        activation='relu'))\n",
    "   \n",
    "    model.add(Dense(1, activation='sigmoid'))\n",
    "    \n",
    "    #adam_opt = tf.optimizers.Adam(lr=0.001, beta_1=0.9, beta_2=0.999, epsilon=None, decay=0.0, amsgrad=False)\n",
    "    model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['AUC', 'acc'])\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71e6a37a",
   "metadata": {},
   "source": [
    "## Tuner"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "b75bd353",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Reloading Oracle from existing project .\\tinelstm\\oracle.json\n"
     ]
    }
   ],
   "source": [
    "tuner= BayesianOptimization(       \n",
    "        tune_lstm,\n",
    "        objective='val_acc',\n",
    "        max_trials=20,\n",
    "        executions_per_trial=1,\n",
    "        project_name='tinelstm'\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63c4ed7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# DNN Tuner\n",
    "tuner.search(\n",
    "        x=x_train_numeric,\n",
    "        y=y_train_numeric,\n",
    "        epochs=30,\n",
    "        batch_size=32,\n",
    "        validation_data=(x_test_numeric,y_test_numeric)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "083bdb10",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trial 20 Complete [00h 16m 47s]\n",
      "val_acc: 0.80085688829422\n",
      "\n",
      "Best val_acc So Far: 0.8024436831474304\n",
      "Total elapsed time: 06h 00m 20s\n",
      "INFO:tensorflow:Oracle triggered exit\n"
     ]
    }
   ],
   "source": [
    "# RNN Tuner\n",
    "tuner.search(\n",
    "        x=X_train,\n",
    "        y=y_train,\n",
    "        epochs=20,\n",
    "        batch_size=128,\n",
    "        validation_data=(X_test,y_test)\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd794449",
   "metadata": {},
   "source": [
    "### Tuner Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db04faa0",
   "metadata": {},
   "outputs": [],
   "source": [
    "tuner.search_space_summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3752d7c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "tuner.results_summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "5a3a863c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_unit': 352,\n",
       " 'n_layers': 1,\n",
       " 'lstm_0_units': 512,\n",
       " 'layer_final': 128,\n",
       " 'Dropout_rate': 0.2,\n",
       " 'dense_activation': 'sigmoid',\n",
       " 'lstm_1_units': 416}"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tuner.get_best_hyperparameters()[0].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85fb64dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "tuner.get_best_models()[0].summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00f1c4f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "best_model = tuner.get_best_models(num_models=1)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a87224d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "save_model(best_model,'GRU_v1.h5')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
